{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative Adversarial Network #2 (DCGAN)\n",
    "Reference: [Understanding and building Adversarial Networks (GANs) - Deep Learning with PyTorch](https://becominghuman.ai/understanding-and-building-generative-adversarial-networks-gans-8de7c1dc0e25)\n",
    "\n",
    "![GAN Concept](https://cdn-images-1.medium.com/max/1600/1*YH3b1fARO-bf6gU3kyzT4A.jpeg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing required libraries\n",
    "from __future__ import print_function\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting hyperparameters\n",
    "batchSize = 64 \n",
    "imageSize = 64    # Size of images generated: (64 x 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the transformations\n",
    "transform = transforms.Compose([transforms.Resize(imageSize),    # transforms.Scale will be deprecated.\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Loading the dataset\n",
    "dataset = dset.CIFAR10(root = './data', download = True, transform = transform)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size = batchSize, shuffle = True, num_workers = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining a universal function to initialise the weights\n",
    "def weights_init(m):    # 'm' is a neural network\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        m.weight.data.normal_(0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.fill_(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outline of training process\n",
    "\n",
    "![Training Process](https://cdn-images-1.medium.com/max/1600/1*NFO8IogPJRf_eGKBZnd-Fg.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator class\n",
    "class G(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(G, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.ConvTranspose2d(100, 512, 4, 1, 0, bias = False),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(512, 256, 4, 2, 1, bias = False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(256, 128, 4, 2, 1, bias = False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(128, 64, 4, 2, 1, bias = False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(64, 3, 4, 2, 1, bias = False),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "    \n",
    "    def forward(self, input):\n",
    "        output = self.main(input)    # Input will be random vector of size 100\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "G(\n",
       "  (main): Sequential(\n",
       "    (0): ConvTranspose2d(100, 512, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
       "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace)\n",
       "    (3): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU(inplace)\n",
       "    (6): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (8): ReLU(inplace)\n",
       "    (9): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (10): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (11): ReLU(inplace)\n",
       "    (12): ConvTranspose2d(64, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (13): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create Generator\n",
    "netG = G() \n",
    "netG.apply(weights_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class D(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(D, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 4, 2, 1, bias = False),\n",
    "            nn.LeakyReLU(0.2, inplace = True),\n",
    "            nn.Conv2d(64, 128, 4, 2, 1, bias = False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, inplace = True),\n",
    "            nn.Conv2d(128, 256, 4, 2, 1, bias = False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2, inplace = True),\n",
    "            nn.Conv2d(256, 512, 4, 2, 1, bias = False),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2, inplace = True),\n",
    "            nn.Conv2d(512, 1, 4, 1, 0, bias = False),    # Output will be between 0 and 1\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, input):\n",
    "        output = self.main(input)\n",
    "        return output.view(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "D(\n",
       "  (main): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (1): LeakyReLU(negative_slope=0.2, inplace)\n",
       "    (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (4): LeakyReLU(negative_slope=0.2, inplace)\n",
       "    (5): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (7): LeakyReLU(negative_slope=0.2, inplace)\n",
       "    (8): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (9): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (10): LeakyReLU(negative_slope=0.2, inplace)\n",
       "    (11): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
       "    (12): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create Discriminator\n",
    "netD = D() \n",
    "netD.apply(weights_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error prediction measurement\n",
    "criterion = nn.BCELoss()    # BCE: Binary Cross Entropy\n",
    "optimizerD = optim.Adam(netD.parameters(), lr = 0.0002, betas = (0.5, 0.999))\n",
    "optimizerG = optim.Adam(netG.parameters(), lr = 0.0002, betas = (0.5, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/25][0/782]    Loss_D: 2.1004      Loss_G: 2.4990      Time Cost: 3.898 sec\n",
      "[0/25][250/782]    Loss_D: 0.1520      Loss_G: 5.1958      Time Cost: 3.339 sec\n",
      "[0/25][500/782]    Loss_D: 0.5522      Loss_G: 3.5930      Time Cost: 3.344 sec\n",
      "[0/25][750/782]    Loss_D: 0.4159      Loss_G: 3.2064      Time Cost: 3.795 sec\n",
      "[1/25][0/782]    Loss_D: 0.6473      Loss_G: 6.4075      Time Cost: 3.768 sec\n",
      "[1/25][250/782]    Loss_D: 0.3442      Loss_G: 4.0436      Time Cost: 3.582 sec\n",
      "[1/25][500/782]    Loss_D: 1.3188      Loss_G: 3.0625      Time Cost: 3.136 sec\n",
      "[1/25][750/782]    Loss_D: 1.5098      Loss_G: 1.2898      Time Cost: 2.962 sec\n",
      "[2/25][0/782]    Loss_D: 0.6429      Loss_G: 2.6216      Time Cost: 3.127 sec\n",
      "[2/25][250/782]    Loss_D: 0.7551      Loss_G: 5.0210      Time Cost: 3.107 sec\n",
      "[2/25][500/782]    Loss_D: 0.5873      Loss_G: 4.0651      Time Cost: 2.984 sec\n",
      "[2/25][750/782]    Loss_D: 0.5259      Loss_G: 3.4060      Time Cost: 3.021 sec\n",
      "[3/25][0/782]    Loss_D: 0.7840      Loss_G: 4.1562      Time Cost: 3.110 sec\n",
      "[3/25][250/782]    Loss_D: 1.8902      Loss_G: 8.8973      Time Cost: 2.964 sec\n",
      "[3/25][500/782]    Loss_D: 0.7495      Loss_G: 4.5120      Time Cost: 2.912 sec\n",
      "[3/25][750/782]    Loss_D: 0.1871      Loss_G: 3.4975      Time Cost: 3.912 sec\n",
      "[4/25][0/782]    Loss_D: 1.6428      Loss_G: 5.2076      Time Cost: 3.187 sec\n",
      "[4/25][250/782]    Loss_D: 0.3958      Loss_G: 2.5004      Time Cost: 3.492 sec\n",
      "[4/25][500/782]    Loss_D: 0.4027      Loss_G: 2.1375      Time Cost: 3.267 sec\n",
      "[4/25][750/782]    Loss_D: 0.4102      Loss_G: 2.4574      Time Cost: 3.092 sec\n",
      "[5/25][0/782]    Loss_D: 0.9736      Loss_G: 1.2686      Time Cost: 3.115 sec\n",
      "[5/25][250/782]    Loss_D: 0.3388      Loss_G: 2.7765      Time Cost: 2.953 sec\n",
      "[5/25][500/782]    Loss_D: 0.2313      Loss_G: 4.8066      Time Cost: 2.948 sec\n",
      "[5/25][750/782]    Loss_D: 0.3700      Loss_G: 2.4244      Time Cost: 3.248 sec\n",
      "[6/25][0/782]    Loss_D: 0.4250      Loss_G: 2.7028      Time Cost: 3.092 sec\n",
      "[6/25][250/782]    Loss_D: 0.1366      Loss_G: 3.3078      Time Cost: 3.747 sec\n",
      "[6/25][500/782]    Loss_D: 0.2655      Loss_G: 2.7904      Time Cost: 3.014 sec\n",
      "[6/25][750/782]    Loss_D: 0.0308      Loss_G: 4.5112      Time Cost: 3.125 sec\n",
      "[7/25][0/782]    Loss_D: 1.1025      Loss_G: 4.3674      Time Cost: 3.087 sec\n",
      "[7/25][250/782]    Loss_D: 0.0766      Loss_G: 3.9867      Time Cost: 2.986 sec\n",
      "[7/25][500/782]    Loss_D: 0.4987      Loss_G: 2.9516      Time Cost: 3.094 sec\n",
      "[7/25][750/782]    Loss_D: 0.2441      Loss_G: 4.7234      Time Cost: 3.160 sec\n",
      "[8/25][0/782]    Loss_D: 0.1040      Loss_G: 4.6312      Time Cost: 2.584 sec\n",
      "[8/25][250/782]    Loss_D: 0.6594      Loss_G: 2.3186      Time Cost: 3.004 sec\n",
      "[8/25][500/782]    Loss_D: 0.8944      Loss_G: 1.5708      Time Cost: 3.094 sec\n",
      "[8/25][750/782]    Loss_D: 0.0771      Loss_G: 4.4674      Time Cost: 2.944 sec\n",
      "[9/25][0/782]    Loss_D: 0.3248      Loss_G: 4.9496      Time Cost: 2.958 sec\n",
      "[9/25][250/782]    Loss_D: 0.6738      Loss_G: 2.6087      Time Cost: 3.056 sec\n",
      "[9/25][500/782]    Loss_D: 0.4231      Loss_G: 3.7697      Time Cost: 3.132 sec\n",
      "[9/25][750/782]    Loss_D: 0.1260      Loss_G: 6.1916      Time Cost: 2.994 sec\n",
      "[10/25][0/782]    Loss_D: 4.4339      Loss_G: 1.3561      Time Cost: 2.950 sec\n",
      "[10/25][250/782]    Loss_D: 0.3381      Loss_G: 3.5161      Time Cost: 3.099 sec\n",
      "[10/25][500/782]    Loss_D: 0.6995      Loss_G: 1.7815      Time Cost: 3.263 sec\n",
      "[10/25][750/782]    Loss_D: 0.5398      Loss_G: 2.1259      Time Cost: 2.967 sec\n",
      "[11/25][0/782]    Loss_D: 0.1873      Loss_G: 3.9559      Time Cost: 2.960 sec\n",
      "[11/25][250/782]    Loss_D: 0.9171      Loss_G: 3.2672      Time Cost: 3.087 sec\n",
      "[11/25][500/782]    Loss_D: 0.0565      Loss_G: 4.3759      Time Cost: 3.171 sec\n",
      "[11/25][750/782]    Loss_D: 0.9029      Loss_G: 4.3411      Time Cost: 3.051 sec\n",
      "[12/25][0/782]    Loss_D: 0.9423      Loss_G: 1.0065      Time Cost: 2.993 sec\n",
      "[12/25][250/782]    Loss_D: 0.7505      Loss_G: 1.5142      Time Cost: 3.083 sec\n",
      "[12/25][500/782]    Loss_D: 0.1173      Loss_G: 7.8352      Time Cost: 2.646 sec\n",
      "[12/25][750/782]    Loss_D: 0.1274      Loss_G: 3.7539      Time Cost: 3.066 sec\n",
      "[13/25][0/782]    Loss_D: 1.0228      Loss_G: 1.3795      Time Cost: 3.014 sec\n",
      "[13/25][250/782]    Loss_D: 2.4220      Loss_G: 1.3364      Time Cost: 3.171 sec\n",
      "[13/25][500/782]    Loss_D: 0.8562      Loss_G: 2.1552      Time Cost: 2.931 sec\n",
      "[13/25][750/782]    Loss_D: 0.0880      Loss_G: 5.1280      Time Cost: 3.050 sec\n",
      "[14/25][0/782]    Loss_D: 0.0388      Loss_G: 5.4830      Time Cost: 3.047 sec\n",
      "[14/25][250/782]    Loss_D: 0.1346      Loss_G: 4.3350      Time Cost: 3.161 sec\n",
      "[14/25][500/782]    Loss_D: 0.0396      Loss_G: 4.4789      Time Cost: 3.001 sec\n",
      "[14/25][750/782]    Loss_D: 1.9878      Loss_G: 0.4828      Time Cost: 3.029 sec\n",
      "[15/25][0/782]    Loss_D: 0.5652      Loss_G: 2.8936      Time Cost: 3.060 sec\n",
      "[15/25][250/782]    Loss_D: 0.9978      Loss_G: 2.3552      Time Cost: 3.214 sec\n",
      "[15/25][500/782]    Loss_D: 0.8365      Loss_G: 1.6153      Time Cost: 2.964 sec\n",
      "[15/25][750/782]    Loss_D: 0.0680      Loss_G: 4.6302      Time Cost: 4.207 sec\n",
      "[16/25][0/782]    Loss_D: 0.0276      Loss_G: 5.1104      Time Cost: 3.457 sec\n",
      "[16/25][250/782]    Loss_D: 0.6396      Loss_G: 2.1559      Time Cost: 3.243 sec\n",
      "[16/25][500/782]    Loss_D: 0.5761      Loss_G: 6.4926      Time Cost: 3.058 sec\n",
      "[16/25][750/782]    Loss_D: 3.6913      Loss_G: 1.1523      Time Cost: 3.171 sec\n",
      "[17/25][0/782]    Loss_D: 0.6726      Loss_G: 5.4055      Time Cost: 3.165 sec\n",
      "[17/25][250/782]    Loss_D: 0.6206      Loss_G: 2.0317      Time Cost: 2.984 sec\n",
      "[17/25][500/782]    Loss_D: 0.0642      Loss_G: 4.8405      Time Cost: 3.075 sec\n",
      "[17/25][750/782]    Loss_D: 1.0230      Loss_G: 1.3419      Time Cost: 3.200 sec\n",
      "[18/25][0/782]    Loss_D: 0.8580      Loss_G: 1.4010      Time Cost: 3.173 sec\n",
      "[18/25][250/782]    Loss_D: 0.6643      Loss_G: 2.9715      Time Cost: 2.960 sec\n",
      "[18/25][500/782]    Loss_D: 0.1120      Loss_G: 3.7147      Time Cost: 3.079 sec\n",
      "[18/25][750/782]    Loss_D: 0.5152      Loss_G: 2.6517      Time Cost: 3.151 sec\n",
      "[19/25][0/782]    Loss_D: 0.5195      Loss_G: 2.6657      Time Cost: 3.115 sec\n",
      "[19/25][250/782]    Loss_D: 0.0055      Loss_G: 7.7404      Time Cost: 2.970 sec\n",
      "[19/25][500/782]    Loss_D: 0.0024      Loss_G: 7.1338      Time Cost: 3.066 sec\n",
      "[19/25][750/782]    Loss_D: 0.0267      Loss_G: 11.2360     Time Cost: 3.122 sec\n",
      "[20/25][0/782]    Loss_D: 0.3663      Loss_G: 14.7482     Time Cost: 3.055 sec\n",
      "[20/25][250/782]    Loss_D: 0.0402      Loss_G: 5.8704      Time Cost: 2.957 sec\n",
      "[20/25][500/782]    Loss_D: 1.0275      Loss_G: 10.7368     Time Cost: 3.038 sec\n",
      "[20/25][750/782]    Loss_D: 0.2049      Loss_G: 4.2608      Time Cost: 3.148 sec\n",
      "[21/25][0/782]    Loss_D: 0.0776      Loss_G: 4.6620      Time Cost: 3.092 sec\n",
      "[21/25][250/782]    Loss_D: 0.6848      Loss_G: 6.5319      Time Cost: 3.179 sec\n",
      "[21/25][500/782]    Loss_D: 0.4774      Loss_G: 1.5932      Time Cost: 3.037 sec\n",
      "[21/25][750/782]    Loss_D: 0.7053      Loss_G: 1.7250      Time Cost: 3.171 sec\n",
      "[22/25][0/782]    Loss_D: 0.3711      Loss_G: 4.1799      Time Cost: 3.174 sec\n",
      "[22/25][250/782]    Loss_D: 0.1441      Loss_G: 4.4452      Time Cost: 3.021 sec\n",
      "[22/25][500/782]    Loss_D: 0.8618      Loss_G: 1.4911      Time Cost: 3.076 sec\n",
      "[22/25][750/782]    Loss_D: 0.7506      Loss_G: 1.0846      Time Cost: 3.163 sec\n",
      "[23/25][0/782]    Loss_D: 1.3531      Loss_G: 1.5708      Time Cost: 3.141 sec\n",
      "[23/25][250/782]    Loss_D: 0.5683      Loss_G: 1.8841      Time Cost: 2.968 sec\n",
      "[23/25][500/782]    Loss_D: 0.0701      Loss_G: 4.1910      Time Cost: 3.111 sec\n",
      "[23/25][750/782]    Loss_D: 0.4927      Loss_G: 1.8814      Time Cost: 3.184 sec\n",
      "[24/25][0/782]    Loss_D: 0.1732      Loss_G: 3.4092      Time Cost: 3.159 sec\n",
      "[24/25][250/782]    Loss_D: 0.8114      Loss_G: 4.1679      Time Cost: 3.034 sec\n",
      "[24/25][500/782]    Loss_D: 0.5948      Loss_G: 2.5810      Time Cost: 3.140 sec\n",
      "[24/25][750/782]    Loss_D: 0.3301      Loss_G: 3.4158      Time Cost: 3.213 sec\n"
     ]
    }
   ],
   "source": [
    "# Training Start\n",
    "# 25 Epochs\n",
    "\n",
    "for epoch in range(25):    \n",
    "    \n",
    "    # Iterate over images within the dataset\n",
    "    for i, data in enumerate(dataloader, 0):\n",
    "        start = time.time()\n",
    "        \n",
    "        ### TRAIN DISCRIMINATOR ###\n",
    "        # Initialize discriminator weights\n",
    "        netD.zero_grad()\n",
    "        \n",
    "        # Train the discriminator with the real image dataset first.\n",
    "        real, _ = data\n",
    "        input = Variable(real)\n",
    "        target = Variable(torch.ones(input.size()[0]))\n",
    "        output = netD(input)\n",
    "        errD_real = criterion(output, target)\n",
    "        \n",
    "        # Now, train with fake images, generated by Generator\n",
    "        noise = Variable(torch.randn(input.size()[0], 100, 1, 1))\n",
    "        fake = netG(noise)\n",
    "        target = Variable(torch.zeros(input.size()[0]))\n",
    "        output = netD(fake.detach())\n",
    "        errD_fake = criterion(output, target)\n",
    "        \n",
    "        # Back-propagate the error\n",
    "        errD = errD_real + errD_fake\n",
    "        errD.backward()\n",
    "        optimizerD.step()\n",
    "        \n",
    "        \n",
    "        ### TRAIN GENERATOR ###\n",
    "        # Update generator weights\n",
    "        netG.zero_grad()    # Initialize weights\n",
    "        target = Variable(torch.ones(input.size()[0]))\n",
    "        output = netD(fake)\n",
    "        errG = criterion(output, target)\n",
    "        errG.backward()\n",
    "        optimizerG.step()\n",
    "        \n",
    "        \n",
    "        ## Print the losses and save the real images & the generated images for every 100 steps.\n",
    "        if i % 250 == 0:\n",
    "            end = time.time()\n",
    "            print('[%d/%d][%d/%d]    Loss_D: %-8.4f    Loss_G: %-8.4f    Time Cost: %-6.3fsec' \\\n",
    "                  % (epoch, 25, i, len(dataloader), errD.item(), errG.item(), end - start))    # .data[0] will be deprecated.\n",
    "            start = time.time()\n",
    "            torch.save(netD.state_dict(), \"./models/Discriminator/D_{}_{}.pth\".format(epoch, i))\n",
    "            torch.save(netG.state_dict(), \"./models/Generator/G_{}_{}.pth\".format(epoch, i))\n",
    "        if i % 100 == 0:\n",
    "            vutils.save_image(real, '%s/real_samples.png' % \"./results\", normalize = True)\n",
    "            fake = netG(noise)\n",
    "            vutils.save_image(fake.data, '%s/fake_samples_epoch_%03d.png' % (\"./results\", epoch), normalize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
